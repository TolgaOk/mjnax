{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable, Tuple, Dict, Union, Any, NewType, Sequence\n",
    "from flax import struct\n",
    "from typing import Tuple\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrd\n",
    "import jax\n",
    "from nestedtuple import nestedtuple\n",
    "from gymnax.environments.environment import Environment\n",
    "from jax.typing import ArrayLike as KeyType\n",
    "import distrax\n",
    "\n",
    "import jaxdp\n",
    "from jaxdp.learning.algorithms import q_learning, StepSample\n",
    "from jaxdp.learning.runner import train, reducer\n",
    "from jaxdp.mdp.sampler import SamplerState, RolloutSample, sample_gymnax_rollout\n",
    "from jaxdp.typehints import F, QType\n",
    "\n",
    "from mjnax.pendulum import MjModelType, MjStateType, DiscretizedPendulum\n",
    "\n",
    "\n",
    "@nestedtuple\n",
    "class Arg:\n",
    "    seed: int = 42                     # Initial seeds\n",
    "    n_seed: int = 10                  # Number of seeds to execute the same algorithm\n",
    "    n_env: int = 4                     # Number of parallel environments for sampling\n",
    "\n",
    "    class policy_fn:\n",
    "        epsilon: float = 0.15          # Epsilon-greedy parameter\n",
    "\n",
    "    class update_fn:\n",
    "        alpha: float = 0.10            # Step size (a.k.a learning rate)\n",
    "\n",
    "    class train_loop:\n",
    "        gamma: float = 0.99            # Discount factor\n",
    "        n_steps: int = 1000            # Number of steps\n",
    "        eval_period: int = 50          # Evaluation period (in terms of <n_steps>)\n",
    "\n",
    "    class sampler_init:\n",
    "        queue_size: int = 50           # Queue size of the sampler for the metrics\n",
    "\n",
    "    class sampler_fn:\n",
    "        max_episode_len: int = 125  # Maximum length of an episode allowed by the sampler\n",
    "        rollout_len: int = 10          # Length of a rollout\n",
    "\n",
    "    class value_init:\n",
    "        minval: float = -1.0            # Minimum value of the uniform distribution\n",
    "        maxval: float = 1.0            # Maxiumum value of the uniform distribution\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class EGreedyPolicyState():\n",
    "    value: QType\n",
    "    epsilon: float\n",
    "\n",
    "\n",
    "class EGreedyPolicy():\n",
    "\n",
    "    def sample(self, key, state: EGreedyPolicyState, obs: F[\"S\"]):\n",
    "        pi = jaxdp.e_greedy_policy.q(state.value, state.epsilon)\n",
    "        policy_p = jnp.einsum(\"as,s->a\", pi, obs)\n",
    "        act = jaxdp.sample_from(policy_p, key)\n",
    "        return act, state\n",
    "\n",
    "    @staticmethod\n",
    "    def reset(key: KeyType, state: EGreedyPolicyState) -> EGreedyPolicyState:\n",
    "        return state\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class Metric():\n",
    "    td_error: F[\"N\"]\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class RunState():\n",
    "    key: jax.Array\n",
    "    sampler: SamplerState\n",
    "    env_state: MjStateType\n",
    "    env_model: MjModelType\n",
    "    pi: EGreedyPolicyState\n",
    "    metric: Metric\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class RunStatic():\n",
    "    env: Environment\n",
    "    pi: EGreedyPolicy\n",
    "    logger: Callable\n",
    "    sampler: Callable\n",
    "    updater: Callable\n",
    "\n",
    "\n",
    "def reset_metric(size: int) -> Metric:\n",
    "    return Metric(\n",
    "        *(jnp.full(size, jnp.nan)\n",
    "          for _ in Metric.__dataclass_fields__)\n",
    "    )\n",
    "\n",
    "\n",
    "# @partial(jax.jit, static_argnums=[1, 2])\n",
    "def sample_batch_rollout(state: RunState, static: RunStatic, arg: Arg\n",
    "                         ) -> Tuple[RolloutSample, RunState]:\n",
    "    key, sample_key = jrd.split(state.key)\n",
    "    # jax.debug.print(\"{x}\", x=(state.sampler.last_obs.shape, state.pi.value.shape))\n",
    "    (\n",
    "        _, rollout, sampler_state, env_state, policy_state\n",
    "    ) = jax.vmap(partial(sample_gymnax_rollout,\n",
    "                         env=static.env,\n",
    "                         policy=static.pi,\n",
    "                         rollout_length=arg.sampler_fn.rollout_len,\n",
    "                         max_episode_length=arg.sampler_fn.max_episode_len),\n",
    "                 in_axes=(0, 0, 0, None, None),\n",
    "                 out_axes=(0, 0, 0, 0, EGreedyPolicyState(value=None, epsilon=None)))(\n",
    "        jrd.split(sample_key, arg.n_env),\n",
    "        state.sampler,\n",
    "        state.env_state,\n",
    "        state.pi,\n",
    "        state.env_model\n",
    "    )\n",
    "    state = state.replace(\n",
    "        key=key,\n",
    "        sampler=sampler_state,\n",
    "        env_state=env_state,\n",
    "        pi=policy_state\n",
    "    )\n",
    "    return rollout, state\n",
    "\n",
    "\n",
    "# @partial(jax.jit, static_argnums=[2, 3])\n",
    "def update_ql(rollout: RolloutSample,\n",
    "              state: RunState,\n",
    "              static: RunStatic,\n",
    "              arg: Arg\n",
    "              ) -> Tuple[RunState, Dict[str, Any]]:\n",
    "\n",
    "    def batch_update_fn(rollout: RolloutSample, pi_state: EGreedyPolicyState) -> EGreedyPolicyState:\n",
    "        batch_step_fn = jax.vmap(jax.vmap(q_learning.asynchronous.step,\n",
    "                                 (0, None, None)), (0, None, None))\n",
    "        scalar_target_values = batch_step_fn(rollout, pi_state.value, arg.train_loop.gamma)\n",
    "        target_value = reducer.every_visit(rollout, scalar_target_values)\n",
    "        updated_value = q_learning.update(\n",
    "            pi_state.value, target_value, alpha=arg.update_fn.alpha\n",
    "        )\n",
    "        return pi_state.replace(value=updated_value), jnp.abs(scalar_target_values).mean()\n",
    "\n",
    "\n",
    "    updated_pi, avg_tde = batch_update_fn(\n",
    "        StepSample(\n",
    "            state=rollout.obs,\n",
    "            next_state=rollout.next_obs,\n",
    "            action=rollout.action,\n",
    "            reward=rollout.reward,\n",
    "            terminal=rollout.terminal,\n",
    "            timeout=rollout.timeout,\n",
    "        ),\n",
    "        state.pi)\n",
    "    return state.replace(pi=updated_pi), {\"td_error\": avg_tde}\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=[2, 3])\n",
    "def train_step(step: int, state: RunState, static: RunStatic, arg: Arg):\n",
    "    rollout, state = static.sampler(state, static, arg)\n",
    "\n",
    "    state, losses = static.updater(rollout, state, static, arg)\n",
    "\n",
    "    metric = state.metric\n",
    "    metric = metric.replace(**{\n",
    "        loss_name: getattr(metric, loss_name).at[step % arg.train_loop.eval_period].set(loss_val)\n",
    "        for loss_name, loss_val in losses.items()\n",
    "    })\n",
    "    # jax.debug.print(\"td {x}\", x=metric)\n",
    "    # jax.debug.print(\"index {x}\", x=step % arg.train_loop.eval_period)\n",
    "\n",
    "    is_log_step = (step % arg.train_loop.eval_period) == (arg.train_loop.eval_period - 1)\n",
    "\n",
    "    def _log(_is_log_step, state, *args):\n",
    "        jax.debug.callback(\n",
    "            lambda _is_log_step, *_args: static.logger(*_args) if _is_log_step else None,\n",
    "            _is_log_step, state, *args)\n",
    "        return {\"metric\": reset_metric(arg.train_loop.eval_period),\n",
    "                \"sampler\": state.sampler.refresh_queues()}\n",
    "\n",
    "    return state.replace(**jax.lax.cond(\n",
    "        is_log_step,\n",
    "        _log,\n",
    "        lambda _, state, __, metric, : {\"metric\": metric, \"sampler\": state.sampler},\n",
    "        is_log_step, state, step, metric\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tok/miniconda3/envs/jaxtor/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:118: UserWarning: Explicitly requested dtype float requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "        Training Metrics - Step          50\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |          0.0000\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |          0.0000\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.0884\n",
      "===========================================\n",
      "        Training Metrics - Step          100\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |          0.0000\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |          0.0000\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.0135\n",
      "===========================================\n",
      "        Training Metrics - Step          150\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |          0.0087\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |          0.1248\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.0238\n",
      "===========================================\n",
      "        Training Metrics - Step          200\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         16.0951\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         25.1776\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.2102\n",
      "===========================================\n",
      "        Training Metrics - Step          250\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         52.6328\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         25.1615\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.6414\n",
      "===========================================\n",
      "        Training Metrics - Step          300\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         51.1404\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         26.0376\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.7488\n",
      "===========================================\n",
      "        Training Metrics - Step          350\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         72.4474\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         17.9887\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.9249\n",
      "===========================================\n",
      "        Training Metrics - Step          400\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         68.2388\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         17.6146\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.9992\n",
      "===========================================\n",
      "        Training Metrics - Step          450\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         52.8480\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         28.6404\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          1.0064\n",
      "===========================================\n",
      "        Training Metrics - Step          500\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         60.4748\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         27.8974\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.8991\n",
      "===========================================\n",
      "        Training Metrics - Step          550\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         54.5862\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         25.6457\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.9226\n",
      "===========================================\n",
      "        Training Metrics - Step          600\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         65.8957\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         21.4602\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.9100\n",
      "===========================================\n",
      "        Training Metrics - Step          650\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         60.0234\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         25.4874\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.9478\n",
      "===========================================\n",
      "        Training Metrics - Step          700\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         71.7659\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         16.8724\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.9340\n",
      "===========================================\n",
      "        Training Metrics - Step          750\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         60.4891\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         25.0868\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.9527\n",
      "===========================================\n",
      "        Training Metrics - Step          800\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         48.4307\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         29.6076\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.8889\n",
      "===========================================\n",
      "        Training Metrics - Step          850\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |         38.8631\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         33.4406\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.7292\n",
      "===========================================\n",
      "        Training Metrics - Step          900\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |          4.4686\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |         12.9017\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.5001\n",
      "===========================================\n",
      "        Training Metrics - Step          950\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |          0.0000\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |          0.0000\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.3519\n",
      "===========================================\n",
      "        Training Metrics - Step          1000\n",
      "-------------------------------------------\n",
      "Mean Behavior Reward      |          0.5760\n",
      "Mean Behavior Length      |        125.0000\n",
      "Std Behavior Reward       |          5.8555\n",
      "Std Behavior Length       |          0.0000\n",
      "Mean Td Error             |          0.2273\n"
     ]
    }
   ],
   "source": [
    "@partial(jax.jit, static_argnums=[])\n",
    "def eval_policy():\n",
    "    \"\"\" TODO: Implement one step eval episode \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def logger(state: RunState, step: int, metric: Metric) -> None:\n",
    "    rewards = state.sampler.episode_reward_queue\n",
    "    lengths = state.sampler.episode_length_queue\n",
    "    values = {\n",
    "        \"mean_behavior_reward\": jnp.nanmean(rewards),\n",
    "        \"mean_behavior_length\": jnp.nanmean(lengths),\n",
    "        \"std_behavior_reward\": jnp.nanstd(rewards),\n",
    "        \"std_behavior_length\": jnp.nanstd(lengths),\n",
    "        \"mean_td_error\": jnp.nanmean(metric.td_error),\n",
    "        # \"debug\": state.behavior_pi.param[\"weight\"].std()\n",
    "    }\n",
    "    title = \"Training Metrics - Step\"\n",
    "    print(\"=\" * 43)\n",
    "    print(f\"{title:^40} {step + 1}\")\n",
    "    print(\"-\" * 43)\n",
    "    for name, val in values.items():\n",
    "        formatted_name = name.replace(\"_\", \" \").title()\n",
    "        print(f\"{formatted_name:<25} | {val:>15.4f}\")\n",
    "\n",
    "\n",
    "arg = Arg(\n",
    "    n_seed=1,\n",
    "    n_env=16,\n",
    "    policy_fn=Arg.policy_fn(epsilon=0.25),\n",
    "    sampler_fn=Arg.sampler_fn(rollout_len=32),\n",
    "    train_loop=Arg.train_loop(eval_period=50, n_steps=1000, gamma=0.99),\n",
    "    update_fn=\n",
    ")\n",
    "\n",
    "env = DiscretizedPendulum()\n",
    "env_model = env.default_params\n",
    "key, env_reset_key, pi_reset_key = jrd.split(jrd.PRNGKey(42), 3)\n",
    "obs, env_state = jax.vmap(env.reset)(jrd.split(env_reset_key, arg.n_env))\n",
    "\n",
    "sampler_state = jax.vmap(SamplerState.initialize_rollout_state, in_axes=(0, None)\n",
    "                         )(obs, arg.sampler_init.queue_size)\n",
    "pi_state = EGreedyPolicyState(\n",
    "    value=jrd.uniform(pi_reset_key, (env.num_actions, env.num_states,),\n",
    "                      dtype=\"float32\", **arg.value_init._asdict()),\n",
    "    epsilon=arg.policy_fn.epsilon)\n",
    "policy = EGreedyPolicy()\n",
    "\n",
    "run_state = RunState(\n",
    "    key,\n",
    "    sampler_state,\n",
    "    env_state,\n",
    "    env_model,\n",
    "    pi_state,\n",
    "    reset_metric(arg.train_loop.eval_period)\n",
    ")\n",
    "\n",
    "run_static = RunStatic(\n",
    "    env,\n",
    "    policy,\n",
    "    logger,\n",
    "    sample_batch_rollout,\n",
    "    update_ql\n",
    ")\n",
    "\n",
    "final_state = jax.lax.fori_loop(\n",
    "    0,\n",
    "    arg.train_loop.n_steps,\n",
    "    partial(train_step, static=run_static, arg=arg),\n",
    "    run_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([4., 4., 4.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "\n",
    "def jax_static_method(method):\n",
    "\n",
    "    def static_method(self, *args, **kwargs):\n",
    "        return method(*args, **kwargs)\n",
    "    \n",
    "    return static_method\n",
    "\n",
    "class X():\n",
    "\n",
    "    # @jax_static_method\n",
    "    def fn(x, y):\n",
    "        return x * y\n",
    "    \n",
    "X.fn(jnp.ones(3), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 4, 'y': 6.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class X(NamedTuple):\n",
    "    x: int = 4\n",
    "    y: float = 6.\n",
    "\n",
    "\n",
    "X()._asdict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxtor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
