{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable, Tuple, Dict, Union, Any, NewType, Sequence\n",
    "from flax import struct\n",
    "from typing import Tuple\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrd\n",
    "import jax\n",
    "from nestedtuple import nestedtuple\n",
    "from gymnax.environments.environment import Environment\n",
    "from jax.typing import ArrayLike as KeyType\n",
    "import distrax\n",
    "\n",
    "import jaxdp\n",
    "from jaxdp.learning.algorithms import q_learning, StepSample\n",
    "from jaxdp.learning.runner import train, reducer\n",
    "from jaxdp.mdp.sampler import SamplerState, RolloutSample, sample_gymnax_rollout\n",
    "from jaxdp.typehints import F, QType\n",
    "\n",
    "from mjnax.pendulum import MjModelType, MjStateType, DiscretizedPendulum\n",
    "\n",
    "\n",
    "@nestedtuple\n",
    "class Arg:\n",
    "    seed: int = 42                     # Initial seeds\n",
    "    n_seed: int = 10                  # Number of seeds to execute the same algorithm\n",
    "    n_env: int = 4                     # Number of parallel environments for sampling\n",
    "\n",
    "    class policy_fn:\n",
    "        epsilon: float = 0.15          # Epsilon-greedy parameter\n",
    "\n",
    "    class update_fn:\n",
    "        alpha: float = 0.10            # Step size (a.k.a learning rate)\n",
    "\n",
    "    class train_loop:\n",
    "        gamma: float = 0.99            # Discount factor\n",
    "        n_steps: int = 1000            # Number of steps\n",
    "\n",
    "    class evaluation:\n",
    "        period: int = 50          # Evaluation period (in terms of <n_steps>)\n",
    "        n_env: int = 10\n",
    "        n_step: int = 250\n",
    "        queue_size: int = 10\n",
    "\n",
    "    class sampler_init:\n",
    "        queue_size: int = 50           # Queue size of the sampler for the metrics\n",
    "\n",
    "    class sampler_fn:\n",
    "        max_episode_len: int = 125  # Maximum length of an episode allowed by the sampler\n",
    "        rollout_len: int = 10          # Length of a rollout\n",
    "\n",
    "    class value_init:\n",
    "        minval: float = -1.0            # Minimum value of the uniform distribution\n",
    "        maxval: float = 1.0            # Maxiumum value of the uniform distribution\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class EGreedyPolicyState():\n",
    "    value: QType\n",
    "    epsilon: float\n",
    "\n",
    "\n",
    "class EGreedyPolicy():\n",
    "\n",
    "    def sample(self, key, state: EGreedyPolicyState, obs: F[\"S\"]):\n",
    "        pi = jaxdp.e_greedy_policy.q(state.value, state.epsilon)\n",
    "        policy_p = jnp.einsum(\"as,s->a\", pi, obs)\n",
    "        act = jaxdp.sample_from(policy_p, key)\n",
    "        return act, state\n",
    "\n",
    "    @staticmethod\n",
    "    def reset(key: KeyType, state: EGreedyPolicyState) -> EGreedyPolicyState:\n",
    "        return state\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class Metric():\n",
    "    td_error: F[\"N\"]\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class RunState():\n",
    "    key: jax.Array\n",
    "    sampler: SamplerState\n",
    "    env_state: MjStateType\n",
    "    env_model: MjModelType\n",
    "    pi: EGreedyPolicyState\n",
    "    metric: Metric\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class RunStatic():\n",
    "    env: Environment\n",
    "    pi: EGreedyPolicy\n",
    "    logger: Callable\n",
    "    sampler: Callable\n",
    "    updater: Callable\n",
    "\n",
    "\n",
    "def reset_metric(size: int) -> Metric:\n",
    "    return Metric(\n",
    "        *(jnp.full(size, jnp.nan)\n",
    "          for _ in Metric.__dataclass_fields__)\n",
    "    )\n",
    "\n",
    "\n",
    "# @partial(jax.jit, static_argnums=[1, 2])\n",
    "def sample_batch_rollout(state: RunState, static: RunStatic, arg: Arg\n",
    "                         ) -> Tuple[RolloutSample, RunState]:\n",
    "    key, sample_key = jrd.split(state.key)\n",
    "    # jax.debug.print(\"{x}\", x=(state.sampler.last_obs.shape, state.pi.value.shape))\n",
    "    (\n",
    "        _, rollout, sampler_state, env_state, policy_state\n",
    "    ) = jax.vmap(partial(sample_gymnax_rollout,\n",
    "                         env=static.env,\n",
    "                         policy=static.pi,\n",
    "                         rollout_length=arg.sampler_fn.rollout_len,\n",
    "                         max_episode_length=arg.sampler_fn.max_episode_len),\n",
    "                 in_axes=(0, 0, 0, None, None),\n",
    "                 out_axes=(0, 0, 0, 0, EGreedyPolicyState(value=None, epsilon=None)))(\n",
    "        jrd.split(sample_key, arg.n_env),\n",
    "        state.sampler,\n",
    "        state.env_state,\n",
    "        state.pi,\n",
    "        state.env_model\n",
    "    )\n",
    "    state = state.replace(\n",
    "        key=key,\n",
    "        sampler=sampler_state,\n",
    "        env_state=env_state,\n",
    "        pi=policy_state\n",
    "    )\n",
    "    return rollout, state\n",
    "\n",
    "\n",
    "# @partial(jax.jit, static_argnums=[2, 3])\n",
    "def update_ql(rollout: RolloutSample,\n",
    "              state: RunState,\n",
    "              static: RunStatic,\n",
    "              arg: Arg\n",
    "              ) -> Tuple[RunState, Dict[str, Any]]:\n",
    "\n",
    "    def batch_update_fn(rollout: RolloutSample, pi_state: EGreedyPolicyState) -> EGreedyPolicyState:\n",
    "        batch_step_fn = jax.vmap(jax.vmap(q_learning.asynchronous.step,\n",
    "                                 (0, None, None)), (0, None, None))\n",
    "        scalar_target_values = batch_step_fn(rollout, pi_state.value, arg.train_loop.gamma)\n",
    "        target_value = reducer.every_visit(rollout, scalar_target_values)\n",
    "        updated_value = q_learning.update(\n",
    "            pi_state.value, target_value, alpha=arg.update_fn.alpha\n",
    "        )\n",
    "        return pi_state.replace(value=updated_value), jnp.abs(scalar_target_values).mean()\n",
    "\n",
    "    updated_pi, avg_tde = batch_update_fn(\n",
    "        StepSample(\n",
    "            state=rollout.obs,\n",
    "            next_state=rollout.next_obs,\n",
    "            action=rollout.action,\n",
    "            reward=rollout.reward,\n",
    "            terminal=rollout.terminal,\n",
    "            timeout=rollout.timeout,\n",
    "        ),\n",
    "        state.pi)\n",
    "    return state.replace(pi=updated_pi), {\"td_error\": avg_tde}\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=[2, 3])\n",
    "def train_step(step: int, state: RunState, static: RunStatic, arg: Arg):\n",
    "    rollout, state = static.sampler(state, static, arg)\n",
    "\n",
    "    state, losses = static.updater(rollout, state, static, arg)\n",
    "\n",
    "    metric = state.metric\n",
    "    metric = metric.replace(**{\n",
    "        loss_name: getattr(metric, loss_name).at[step % arg.evaluation.period].set(loss_val)\n",
    "        for loss_name, loss_val in losses.items()\n",
    "    })\n",
    "    # jax.debug.print(\"td {x}\", x=metric)\n",
    "    # jax.debug.print(\"index {x}\", x=step % arg.evaluation.period)\n",
    "\n",
    "    is_log_step = (step % arg.evaluation.period) == (arg.evaluation.period - 1)\n",
    "\n",
    "    def _log(_is_log_step, state, *args):\n",
    "        jax.debug.callback(\n",
    "            lambda _is_log_step, *_args: partial(static.logger, static=static, arg=arg\n",
    "                                                 )(*_args) if _is_log_step else None,\n",
    "            _is_log_step, state, *args)\n",
    "        return {\"metric\": reset_metric(arg.evaluation.period),\n",
    "                \"sampler\": state.sampler.refresh_queues()}\n",
    "\n",
    "    return state.replace(**jax.lax.cond(\n",
    "        is_log_step,\n",
    "        _log,\n",
    "        lambda _, state, __, metric, : {\"metric\": metric, \"sampler\": state.sampler},\n",
    "        is_log_step, state, step, metric\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tok/miniconda3/envs/jaxtor/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:118: UserWarning: Explicitly requested dtype float requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n"
     ]
    }
   ],
   "source": [
    "@partial(jax.jit, static_argnums=[1, 2])\n",
    "def eval_policy(state: RunState, static: RunStatic, arg: Arg):\n",
    "    \"\"\"Run n eval episodes with the target(greedy) policy\"\"\"\n",
    "\n",
    "    @jax.vmap\n",
    "    def eval_episodes(key):\n",
    "        obs, env_state = env.reset_env(run_state.key, state.env_model)\n",
    "        sampler_state = SamplerState.initialize_rollout_state(\n",
    "            obs, arg.evaluation.queue_size)\n",
    "        _, _, sampler_state, *_ = sample_gymnax_rollout(\n",
    "            key,\n",
    "            sampler_state,\n",
    "            env_state,\n",
    "            state.pi.replace(epsilon=0.0),\n",
    "            state.env_model,\n",
    "            env=static.env,\n",
    "            policy=static.pi,\n",
    "            rollout_length=arg.evaluation.n_step,\n",
    "            max_episode_length=arg.sampler_fn.max_episode_len)\n",
    "        return sampler_state\n",
    "\n",
    "    sampler_states = eval_episodes(jrd.split(state.key, arg.evaluation.n_env))\n",
    "    return {\"mean_target_policy_reward\": jnp.nanmean(sampler_states.episode_reward_queue),\n",
    "            \"mean_target_policy_length\": jnp.nanmean(sampler_states.episode_length_queue)}\n",
    "\n",
    "\n",
    "def logger(state: RunState, step: int, metric: Metric, static: RunStatic, arg: Arg) -> None:\n",
    "    rewards = state.sampler.episode_reward_queue\n",
    "    lengths = state.sampler.episode_length_queue\n",
    "    values = {\n",
    "        \"mean_behavior_reward\": jnp.nanmean(rewards),\n",
    "        \"mean_behavior_length\": jnp.nanmean(lengths),\n",
    "        \"std_behavior_reward\": jnp.nanstd(rewards),\n",
    "        \"std_behavior_length\": jnp.nanstd(lengths),\n",
    "        \"mean_td_error\": jnp.nanmean(metric.td_error),\n",
    "        # \"debug\": state.behavior_pi.param[\"weight\"].std()\n",
    "        **eval_policy(state, static, arg)\n",
    "    }\n",
    "    title = \"Training Metrics - Step\"\n",
    "    print(\"=\" * 43)\n",
    "    print(f\"{title:^40} {step + 1}\")\n",
    "    print(\"-\" * 43)\n",
    "    for name, val in values.items():\n",
    "        formatted_name = name.replace(\"_\", \" \").title()\n",
    "        print(f\"{formatted_name:<25} | {val:>15.4f}\")\n",
    "\n",
    "\n",
    "arg = Arg(\n",
    "    n_seed=1,\n",
    "    n_env=16,\n",
    "    policy_fn=Arg.policy_fn(epsilon=0.15),\n",
    "    sampler_fn=Arg.sampler_fn(rollout_len=32),\n",
    "    train_loop=Arg.train_loop(n_steps=1000, gamma=0.99),\n",
    "    evaluation=Arg.evaluation(),\n",
    "    update_fn=Arg.update_fn(alpha=0.1)\n",
    ")\n",
    "\n",
    "env = DiscretizedPendulum()\n",
    "env_model = env.default_params\n",
    "key, env_reset_key, pi_reset_key = jrd.split(jrd.PRNGKey(42), 3)\n",
    "obs, env_state = jax.vmap(env.reset)(jrd.split(env_reset_key, arg.n_env))\n",
    "\n",
    "sampler_state = jax.vmap(SamplerState.initialize_rollout_state, in_axes=(0, None)\n",
    "                         )(obs, arg.sampler_init.queue_size)\n",
    "pi_state = EGreedyPolicyState(\n",
    "    value=jrd.uniform(pi_reset_key, (env.num_actions, env.num_states,),\n",
    "                      dtype=\"float32\", **arg.value_init._asdict()),\n",
    "    epsilon=arg.policy_fn.epsilon)\n",
    "policy = EGreedyPolicy()\n",
    "\n",
    "run_state = RunState(\n",
    "    key,\n",
    "    sampler_state,\n",
    "    env_state,\n",
    "    env_model,\n",
    "    pi_state,\n",
    "    reset_metric(arg.evaluation.period)\n",
    ")\n",
    "\n",
    "run_static = RunStatic(\n",
    "    env,\n",
    "    policy,\n",
    "    logger,\n",
    "    sample_batch_rollout,\n",
    "    update_ql\n",
    ")\n",
    "\n",
    "final_state = jax.lax.fori_loop(\n",
    "    0,\n",
    "    arg.train_loop.n_steps,\n",
    "    partial(train_step, static=run_static, arg=arg),\n",
    "    run_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([4., 4., 4.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "\n",
    "def jax_static_method(method):\n",
    "\n",
    "    def static_method(self, *args, **kwargs):\n",
    "        return method(*args, **kwargs)\n",
    "\n",
    "    return static_method\n",
    "\n",
    "\n",
    "class X():\n",
    "\n",
    "    # @jax_static_method\n",
    "    def fn(x, y):\n",
    "        return x * y\n",
    "\n",
    "\n",
    "X.fn(jnp.ones(3), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 4, 'y': 6.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class X(NamedTuple):\n",
    "    x: int = 4\n",
    "    y: float = 6.\n",
    "\n",
    "\n",
    "X()._asdict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxtor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
